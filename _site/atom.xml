<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

 <title>Analytic Cravings</title>
 <link href="https://vyaas.github.io/atom.xml" rel="self"/>
 <link href="https://vyaas.github.io/"/>
 <updated>2015-12-29T13:16:00-08:00</updated>
 <id>https://vyaas.github.io</id>
 <author>
   <name>Vyaas Gururajan</name>
   <email></email>
 </author>

 
 <entry>
   <title>Frustration with Fluctuations</title>
   <link href="https://vyaas.github.io/2015/12/25/Frustration-with-Fluctuations/"/>
   <updated>2015-12-25T00:00:00-08:00</updated>
   <id>https://vyaas.github.io/2015/12/25/Frustration-with-Fluctuations</id>
   <content type="html">&lt;p&gt;Studying statistical mechanics has always been a source of pleasure for me, but some of its conceptual hurdles have proven to be quite painful. I hope to convey this struggle in a series of sporadic posts starting with this one.&lt;/p&gt;

&lt;p&gt;The following is a simple, yet radically important point made by &lt;a href=&quot;http://bayes.wustl.edu/&quot;&gt;E.T. Jaynes&lt;/a&gt;. I hope I haven&amp;#39;t butchered any of the nuances.&lt;/p&gt;

&lt;p&gt;When a physical occurrence appears too complicated to understand, we assign to it a probability of happening, based on all the available information we have. These probability distributions are, as Jaynes has put it, carriers of information. They are epistemological things, i.e. they are founded on what we know. For instance, the &lt;em&gt;expectation value&lt;/em&gt; of a quantity \(f\) relevant to some phenonmenon is given by&lt;/p&gt;

&lt;p&gt;$$ \langle f \rangle = \sum_{i}^np_if_i $$&lt;/p&gt;

&lt;p&gt;Here, \(n\) is the number of microscopic configurations the system can be in (i.e. subject to the constraints), \(p_i\) is the probability that the system exhibits the \(i\)th occurrence, and \(f_i\) is the value f takes in such an occurrence. We call \(\langle f \rangle\) the expectation value of \(f\). It is a prediction based on all the information we have of the system.&lt;/p&gt;

&lt;p&gt;How much an \(f_i\) deviates from \(\langle f \rangle\) is given by the difference \( f_i - \langle f \rangle \). The expectation value of such a deviation is an indicator of how reliable our expectation value is.&lt;/p&gt;

&lt;p&gt;$$ \Delta^{2}f = \langle (\Delta f)^2 \rangle $$
$$             = \langle (f - \langle f \rangle)^2 \rangle $$
$$             = \langle f^2 - 2f \langle f\rangle + (\langle f \rangle)^2 \rangle $$
$$             = \langle f^2 \rangle - 2\langle f \rangle \langle f\rangle + \langle f \rangle^2 $$
$$             = \langle f^2 \rangle - \langle f \rangle^2 $$&lt;/p&gt;

&lt;p&gt;We take the expectation value of the squares of the deviations because that gives us a positive number; the expectation value of just the deviation can turn out to be zero and thus hide how much each possibility varies from the mean. The quantity \(\Delta^{2}f\) is called the &lt;em&gt;variance&lt;/em&gt; of \(f\). A large variance indicates that out expectation value is not very reliable. More precisely, the ratio of the expectation values to the square root of the variance tells us how good our expectation value is. This number ought to be much less than one if are to make sharp predictions. &lt;/p&gt;

&lt;p&gt;The expectation values and variances are epistemological things as opposed to ontological things, i.e. they are not physical properties of the system, they are simply estimates based on available information. &lt;/p&gt;

&lt;p&gt;Here is however, an ontological thing:&lt;/p&gt;

&lt;p&gt;$$ \bar{f} = \frac{1}{T}\int_{0}^T f(t)dt $$&lt;/p&gt;

&lt;p&gt;\(\bar{f}\) is the time average of the physical quantity \(f\). It can be experimentally measured. This has nothing to do with probabilities. But we can however make a prediction about \(\bar{f}\) by computing its expectation value:&lt;/p&gt;

&lt;p&gt;$$ \langle\bar{f}\rangle = \left\langle \frac{1}{T}\int_{0}^T f(t)dt \right\rangle $$
$$                       = \frac{1}{T}\int_{0}^T \langle f \rangle dt $$&lt;/p&gt;

&lt;p&gt;Now &lt;em&gt;this&lt;/em&gt; is a statement of probabilities. We always hope to match these expectation values with the measured quantities. The way we do this is to systematically accumulate all information relevant to the phenomenon in question and assign probability distributions. We must dutifully update these probabilities with newly available information if we wish to make reliable predictions. This is the essence of the scientific method, what Jaynes has rightly called the &lt;em&gt;Logic of Science&lt;/em&gt;&lt;sup id=&quot;fnref1&quot;&gt;&lt;a href=&quot;#fn1&quot; rel=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt;. &lt;/p&gt;

&lt;p&gt;At equilibrium, \(\langle\bar{f}\rangle = \langle f\rangle\), but in general, equating time averages with expectation values needs to be founded on more assumptions. To drive home the point, Jaynes asks us to calculate the variance of the time average&lt;sup id=&quot;fnref2&quot;&gt;&lt;a href=&quot;#fn2&quot; rel=&quot;footnote&quot;&gt;2&lt;/a&gt;&lt;/sup&gt;, i.e the reliability of our estimate of the time average.&lt;/p&gt;

&lt;p&gt;$$ \Delta^2\bar{f} = \langle\  (\bar{f} - \langle\bar{f}\rangle)^2\  \rangle $$
$$         = \langle\  (\bar{f} - \langle\bar{f}\rangle) (\bar{f} - \langle\bar{f}\rangle)\rangle $$
$$         = \frac{1}{T^2} \left\langle\ \int_{0}^T(f(t_1)-\langle f(t_1) \rangle)dt_1 \int_{0}^T(f(t_2)-\langle f(t_2) \rangle)dt_2\right \rangle $$&lt;/p&gt;

&lt;p&gt;$$         = \frac{1}{T^2} \left\langle\ \int_{0}^T\int_{0}^T(f(t_1)f(t_2)-f(t_1)\langle f(t_2) \rangle - f(t_2)\langle f(t_1)\rangle + \langle f(t_1) \rangle \langle f(t_2) \rangle)dt_1dt_2  \right\rangle $$
$$         = \frac{1}{T^2} \int_{0}^T\int_{0}^T(\langle f(t_1)f(t_2)\rangle - \langle f(t_1) \rangle \langle f(t_2) \rangle)dt_1dt_2  $$&lt;/p&gt;

&lt;p&gt;The integrand is a function of times \(t_1\) and \(t_2\). But, if the system is in a stationary state, then the itegrand, which we call the &lt;em&gt;autocorrellation function&lt;/em&gt;&lt;sup id=&quot;fnref3&quot;&gt;&lt;a href=&quot;#fn3&quot; rel=&quot;footnote&quot;&gt;3&lt;/a&gt;&lt;/sup&gt;, depends only on the time difference \(\tau = t_2 - t_1\).&lt;/p&gt;

&lt;p&gt;$$ K(\tau) = \langle f(t)f(t+\tau) \rangle - \langle f(t) \rangle \langle f(t+\tau) \rangle $$
$$         = \langle f(0)f(\tau) \rangle - \langle f \rangle^2  $$&lt;/p&gt;

&lt;p&gt;We should therefore be able to reduce the double integral to a single integral. We do this by changing the variables of integration. Let us define two new variables&lt;/p&gt;

&lt;p&gt;$$ \tau = t_1 - t_2 $$
$$ \eta = t_1 + t_2 $$&lt;/p&gt;

&lt;p&gt;The relationship between the infinitesimal areas in the two domains is given by:&lt;/p&gt;

&lt;p&gt;$$ ||J|| dt_1dt_2 = d\tau d\eta $$&lt;/p&gt;

&lt;p&gt;where \(||J||\) is the absolute value of the Jacobian (determinant) of the transformation which is given by&lt;/p&gt;

&lt;p&gt;$$ \begin{vmatrix}
\frac{\partial \tau}{\partial t_1}  &amp;amp; \frac{\partial \eta}{\partial t_2}  \\ 
\frac{\partial \tau}{\partial t_1}  &amp;amp; \frac{\partial \eta}{\partial t_2}  \\
\end{vmatrix} $$&lt;/p&gt;

&lt;p&gt;According to our adopted transformation, \(||J|| = 2\).&lt;/p&gt;

&lt;p&gt;&lt;center&gt;&lt;img src = &quot;https://vyaas.github.io/public/images/integral_change_of_variables.png&quot;\&gt;&lt;/center&gt;&lt;/p&gt;

&lt;p&gt;To integrate in the new domain, we need to change the limits of integration. We would like to integrate out over \(\eta\) so that we&amp;#39;re left with an integral over \(\tau\). As can be seen from the figure below, when \(\tau&amp;gt;0\), \(\eta\) varies between \(\tau\) and \(2T-\tau\). When \(\tau&amp;lt;0\), \(\eta\) varies between \(\tau\) and \(2T+\tau\). So on the whole, we see that \(\eta\) varies between \(|\tau|\) and \(2T-|\tau|\). \(\tau\) ofcourse varies between \(-T\) and \(T\). Therefore, &lt;/p&gt;

&lt;p&gt;$$ \Delta^2\bar{f} = \frac{1}{2T^2}\int_{-T}^{T}d\tau\int_{|\tau|}^{2T-|\tau|}K(\tau)d\eta $$
$$                 = \frac{1}{T^2}\int_{-T}^{T}(T-\tau)K(\tau)d\tau $$&lt;/p&gt;

&lt;p&gt;Since \(K(\tau)\) is symmetric in \(\tau\),
$$  \Delta^2\bar{f} = \frac{2}{T^2}\int_{0}^{T}(T-\tau)K(\tau)d\tau \ \ \ \ \ \ \ \ \color{white}{(e.1)}$$&lt;/p&gt;

&lt;p&gt;It is insufficient to claim that the variance of \(\bar{f}\) becomes insignificantly small as \(T\to\infty\), i.e. it is not enough to say that for long averaging times, our estimate of the time average becomes precise. For that to be true, we need the integrals \(\int_{0}^{\infty}K(\tau)d\tau\) and \(\int_{0}^{\infty}\tau K(\tau)d\tau\) to converge. So if correlations persist for a long time, i.e. indefinitely, we cannot make a reliable estimate of the time average! &lt;/p&gt;

&lt;p&gt;What about measurable fluctuations? The root-mean-square fluctuation of a quantity \(f\) is given by&lt;/p&gt;

&lt;p&gt;$$ \delta^2 f = \frac{1}{T}\int_{0}^{T}(f(t)-\bar{f})^2dt $$ 
$$        = \bar{f^2} - (\bar{f})^2 $$&lt;/p&gt;

&lt;p&gt;This is a measure of how much a quantity is different from its time average. It is indeed measurable and has nothing to do with the variance of the quantity or the variance of the time average of the quantity. &lt;/p&gt;

&lt;p&gt;But there is an unexpected relationship between the &lt;em&gt;expectation value&lt;/em&gt; of the RMS fluctuation and the variance of the time average. To derive it we first note that there is a second equivalent formula for the RMS fluctuation of f:&lt;/p&gt;

&lt;p&gt;$$ \delta^2 f = \frac{1}{T}\int_{0}^{T}(f(t)-\bar{f})^2dt $$
$$    = \frac{1}{T^2}\int_{0}^{T}\int_{0}^{T}(f(t_1)^2-f(t_1)(t_2))dt_1dt_2 $$&lt;/p&gt;

&lt;p&gt;Why have we converted this single integral into a double integral? You&amp;#39;ll soon see why. Now let us take the expectation value of both sides:&lt;/p&gt;

&lt;p&gt;$$ \langle \delta^2 f \rangle = \frac{1}{T^2}\int_{0}^{T}\int_{0}^{T}(\langle f(t_1)^2 \rangle - \langle f(t_1)f(t_2) \rangle)dt_1dt_2 $$&lt;/p&gt;

&lt;p&gt;Since we&amp;#39;re discussing stationary processes, we&amp;#39;ll convert this double integral to a single integral using the same change of variables we did earlier. The expectation value of the RMS fluctuation is thus&lt;/p&gt;

&lt;p&gt;$$ \langle \delta^2 f \rangle = \frac{2}{T^2}\int_{0}^{T}(T-\tau)G(\tau)d\tau \ \ \ \ \ \ \ \ \color{white}{(e.2)}$$&lt;/p&gt;

&lt;p&gt;where 
$$ G(\tau) = \langle f^2 \rangle - \langle f(0)f(\tau) \rangle $$&lt;/p&gt;

&lt;p&gt;assuming a stationary process. Note the difference between \(G(\tau)\) and \(K(\tau)\). The latter contains the square of the expectation value while the former contains the expectation value of the square. We know that the difference between these gives us the variance! We can exploit this by rewriting \((e.1) \) as follows:&lt;/p&gt;

&lt;p&gt;$$  \Delta^2\bar{f} = \frac{2}{T^2}\int_{0}^{T}(T-\tau) (\langle f(0)f(\tau) \rangle - \langle f \rangle^2) d\tau $$&lt;/p&gt;

&lt;p&gt;Rearrange this to get &lt;/p&gt;

&lt;p&gt;$$ \frac{2}{T^2}\int_{0}^{T}(T-\tau)\langle f(0)f(\tau) \rangle d\tau = \Delta^2\bar{f} + \langle f \rangle^2 $$&lt;/p&gt;

&lt;p&gt;We can similarly rearrange \((e.2)\) to get 
$$ \frac{2}{T^2}\int_{0}^{T}(T-\tau)\langle f(0)f(\tau) \rangle d\tau = \langle f^2 \rangle  - \langle \delta^2 f \rangle $$&lt;/p&gt;

&lt;p&gt;We therefore have, quite amazingly,&lt;/p&gt;

&lt;p&gt;$$ \langle \delta^2 f \rangle = \Delta^2 f - \Delta^2 \bar{f} $$&lt;/p&gt;

&lt;p&gt;If our time for averaging is long enough that \(\Delta^2 \bar{f} \to 0 \) (i.e., those integrals we talked about need to converge), then we can equate the expectation value of the fluctuation (physical) with the variance (probabilistic)! Note that \( \Delta^2 \bar{f} \leq \Delta^2 f\) because \(\langle \delta^2 f \rangle\) can never be negative.&lt;/p&gt;

&lt;p&gt;And how good is this estimate you ask? To answer that, we need to compute the expectation value of the variance of the variance, which leads to a four-point correlation! This hierarchy is treacherous mathematical territory, but leads one to appreciate the &lt;em&gt;Ergodic problem of probability&lt;/em&gt;. Equating time averages with expectation values is highly non-trivial. The above derivation sheds light on why some fluctuation based results from statistical mechanics make sense, like the Onsager reciprocal relations and the Fluctuation Dissipation Theorem. These have been shown to yield very good agreement with experiment, a reflection of propagating sufficient information via the partition function and also taking time averages over a sufficiently long duration. It is surprising, and often frustrating, that a majority of researchers (and teachers) don&amp;#39;t pay any heed to the above line of reasoning. Part of the problem stems from the prevalent view that probabilities of occurrence are actually frequencies of occurrence. This is, in my opinion and many others, a terribly misinformed view. Nature is not inherently random. If classical and quantum mechanics has taught us anything, it is that these systems are fundamentally deterministic, and that Nature will do whatever it is doing. It is our ignorance in following the detailed motions, the various degrees of freedom, that forces us to resort to probabilistic descriptions. The probabilities we derive in equilibrium statistical mechanics do not take into account the temporal behavior of the system, yet we&amp;#39;re shown results about physical fluctuations and symmetries of kinetic coefficients in the same breath!&lt;/p&gt;

&lt;h1&gt;Onsager&amp;#39;s reasoning&lt;/h1&gt;

&lt;p&gt;So we have &lt;em&gt;some&lt;/em&gt; basis for equating expectation values with time averages, and thus physical fluctuations with probabilistic variances, remembering all the while that correlations need to decay rapidly enough for &amp;quot;long-time&amp;quot; averages to make sense. Even though Onsager doesn&amp;#39;t mention any of this, let us quickly summarize his derivation of the famous reciprocal relations (I&amp;#39;ll use symbols from the &lt;a href=&quot;https://en.wikipedia.org/wiki/Course_of_Theoretical_Physics&quot;&gt;Landafshitz&lt;/a&gt; course). First, we acknowledge Boltzmann&amp;#39;s monumental contribution:&lt;/p&gt;

&lt;p&gt;$$ S = log(\Gamma) $$&lt;/p&gt;

&lt;p&gt;Entropy \((S)\) is the logarithm of the number of microstates \((\Gamma)\) a system could be in for a given macrostate &lt;em&gt;at equilibrium&lt;/em&gt;. Therefore the probability of the system being in a macrostate that is different from equilibrium is given by&lt;/p&gt;

&lt;p&gt;$$ w(x_1,x_2,...,x_n)dx=Ae^{S_e - S(x_1,x_2,...,x_n) }dx $$&lt;/p&gt;

&lt;p&gt;Here, the \(x\)&amp;#39;s are parameters defining some macrostate that the &lt;em&gt;away-from-equilibrium&lt;/em&gt; entropy depends on&lt;sup id=&quot;fnref4&quot;&gt;&lt;a href=&quot;#fn4&quot; rel=&quot;footnote&quot;&gt;4&lt;/a&gt;&lt;/sup&gt;. Since we&amp;#39;re exploring states close to equilibrium, we can expand \(S\) in powers of the \(x\)&amp;#39;s about the equilibrium entropy \(S_e\). Upto the second derivative, we have, in index notation (repeated indices imply sums, \(\delta\)&amp;#39;s are Kronecker):&lt;/p&gt;

&lt;p&gt;$$ w=Ae^{-{\frac{1}{2}\beta_{ik}x_ix_k}} $$&lt;/p&gt;

&lt;p&gt;Equating the integral of the above distribution over all parameters to one gives \(A=\sqrt{\beta}(2\pi)^{-\frac{1}{2}n}\), where \(\beta\) is the determinant of \(\beta_{ik}\). Note that here, we have considered the expectation value of \(x\)&amp;#39;s as 0, so you can think of them as perturbations from their equilibrium values. &lt;/p&gt;

&lt;p&gt;By defining what we&amp;#39;ll call &lt;em&gt;conjugate variables&lt;/em&gt;, given by&lt;/p&gt;

&lt;p&gt;$$ X_i = -\frac{\partial S}{\partial x_i} = \beta_{ik} x_k $$&lt;/p&gt;

&lt;p&gt;, and using the probability distribution \(w\), we can calculate the following expectation values:&lt;/p&gt;

&lt;p&gt;$$ \langle x_i X_k \rangle = \delta_{ik} $$&lt;/p&gt;

&lt;p&gt;$$ \langle x_i x_k \rangle = \beta_{ik}^{-1} $$&lt;/p&gt;

&lt;p&gt;$$ \langle X_i X_k \rangle = \beta_{ik} \ \ \ \ \ \ \color{white}{(e.3)}$$&lt;/p&gt;

&lt;p&gt;The covariance of the conjugate variables is symmetric with respect to the indices \(i\) and \(j\) by virtue of the definition of the covariance matrix \(\beta_{ik}\); it comprises second derivatives of the entropy. So our first obvious symmetry is &lt;/p&gt;

&lt;p&gt;$$ \beta_{ik} = \beta_{ki} \ \ \ \ \ \ \color{white}{(s.1)}  $$&lt;/p&gt;

&lt;p&gt;Having derived the probability distributions for the likelihoods of various quantities (or rather the likelihoods of their deviations from equilibrium), Onsager now asks us to mentally picture &lt;em&gt;actually&lt;/em&gt; perturbing a system from equilibrium, thus introducing &lt;em&gt;time&lt;/em&gt;. This &amp;quot;small&amp;quot; perturbation induces the system to drive itself back to equilibrium. &lt;em&gt;The rate of change of the deviations of the macroscopic quantities from their equilibrium values, is postulated to have the phenomenological form&lt;/em&gt;,&lt;/p&gt;

&lt;p&gt;$$ \dot{x_i} = -\lambda_{ik}x_k $$&lt;/p&gt;

&lt;p&gt;or, since we&amp;#39;ve defined those conjugate variables,&lt;/p&gt;

&lt;p&gt;$$ \dot{x_i} = -\gamma_{ik}X_k \ \ \ \ \ \ \color{white}{(e.4)}$$&lt;/p&gt;

&lt;p&gt;This \(\gamma\) matrix comprises Onsager&amp;#39;s phenomenological &lt;em&gt;kinetic coefficients&lt;/em&gt;. It turns out this matrix is symmetric (that&amp;#39;s the reciprocity!). The implication is that processes approaching equilibrium exhibit a symmetric coupling; for example, if the diffusion of mass transfers heat, the diffusion of heat transfers mass. &lt;/p&gt;

&lt;p&gt;Note that \((e.4)\) guarantees an exponential decay for \(x\), and consequently the correlations \(\langle x_i(t)x_k(t+\tau)\rangle\) as well (recall the discussion about \(K(\tau)\) in footnote 3).&lt;/p&gt;

&lt;p&gt;Onsager then invokes the assumption of &lt;em&gt;microscopic reversibility&lt;/em&gt;. In the time integral \(\langle x_i(t)x_k(t+\tau) \rangle\) (remember that expectation values have been equated with time averages), \(x_i\) is positioned to take place first followed by \(x_k\) at a later time \(\tau\). If all the microscopic trajectories contributing to this integral are symmetric in time (as the laws of classical and quantum mechanics would have it), the relative times at which \(x_i\) and \(x_k\) are positioned in the integral wouldn&amp;#39;t matter. Therefore, our second symmetry is &lt;/p&gt;

&lt;p&gt;$$ \langle x_i(t)x_k(t+\tau) \rangle = \langle x_i(t+\tau)x_k(t)\rangle \ \ \ \ \ \ \color{white}{(s.2)}$$&lt;/p&gt;

&lt;p&gt;(Be apprised that this does not hold if magnetic fields or centripetal forces are present; they don&amp;#39;t reverse direction when time reverses.) Taking derivatives on both sides with respect to \(\tau\) and taking \(\tau\to 0\) gives&lt;/p&gt;

&lt;p&gt;$$ \langle x_i\dot{x_k} \rangle = \langle \dot{x_i}x_k\rangle $$&lt;/p&gt;

&lt;p&gt;We can substitute \((e.4)\) in the above:&lt;/p&gt;

&lt;p&gt;$$ \langle x_i\gamma_{kl}X_l \rangle = \langle \gamma_{il}X_lx_k\rangle $$&lt;/p&gt;

&lt;p&gt;The relationships \((e.3)\) now come in handy since,&lt;/p&gt;

&lt;p&gt;$$ \langle \gamma_{kl}x_iX_l \rangle = \langle \gamma_{il}X_lx_k\rangle $$
$$ \gamma_{kl}\delta_{il} = \gamma_{il}\delta_{lk} $$
$$ \gamma_{ki} = \gamma_{ik} \ \ \ \ \ \ \color{white}{(s.3)}$$&lt;/p&gt;

&lt;p&gt;That last equality is Onsager&amp;#39;s famous reciprocity relation.&lt;/p&gt;

&lt;p&gt;This derivation, as reasonable looking as it is, doesn&amp;#39;t indicate the range of applicability, let alone masking a great deal of non-trivial dynamics by equating expectation values with time averages. The need for introducing phenomenological relations while in the midst of propagating information via probability distributions further embitters any taste. To me, what is most unsettling about it is that entropy, a quantity that is very well defined at equilibrium (by ennumerating all the microstates subject to the experimental constraints), is now seen to &lt;em&gt;evolve&lt;/em&gt; in time &lt;em&gt;outside of equilibrium&lt;/em&gt;! It is possible for a system to take multiple microscopic paths to equilibrium, which the standard definition of entropy says nothing about. Shouldn&amp;#39;t such motions be accounted for in any non-equilibrium description? Herein lies the greatest conceptual controversy/confusion in all of statistical mechanics: Irreversibility and the Second Law of Thermodynamics. Jaynes spent a lifetime demystifying much of this and has left us with a great deal of wisdom on how to articulate and resolve such problems. I will share my notes from studying his works in a future article. &lt;/p&gt;

&lt;h1&gt;Footnotes&lt;/h1&gt;

&lt;div class=&quot;footnotes&quot;&gt;
&lt;hr&gt;
&lt;ol&gt;

&lt;li id=&quot;fn1&quot;&gt;
&lt;p&gt;Reading Jaynes&amp;#39; &lt;a href=&quot;https://books.google.se/books/about/Probability_Theory.html?id=tTN4HuUNXjgC&amp;amp;redir_esc=y&quot;&gt;&lt;em&gt;Probability Theory: Logic of Science&lt;/em&gt;&lt;/a&gt; is an intellectual adventure deserved to be experienced by everyone. Not just people in Science. Everyone.&amp;nbsp;&lt;a href=&quot;#fnref1&quot; rev=&quot;footnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;

&lt;li id=&quot;fn2&quot;&gt;
&lt;p&gt;Jaynes, E. T., 1979, &lt;a href=&quot;http://bayes.wustl.edu/etj/node1.html&quot;&gt;&lt;em&gt;Where do we Stand on Maximum Entropy?&lt;/em&gt;&lt;/a&gt;.&amp;nbsp;&lt;a href=&quot;#fnref2&quot; rev=&quot;footnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;

&lt;li id=&quot;fn3&quot;&gt;
&lt;p&gt;\(K(\tau)\), is a combination of expectation values. It is common practice to replace these expectation values with time averages. We can immediately appreciate the reason for such an impulse: while \(\langle f(0)\rangle\) and \(\langle f(\tau)\rangle\) are predictions that come from equilibrium statistical mechanics, there&amp;#39;s very little we can say about \(\langle f(0) f(\tau)\rangle\) from equilibrium considerations alone. The replacement of expectation values with time averages and vice versa is an assumption called the &lt;em&gt;Ergodic Hypothesis&lt;/em&gt;. Once this is done, one can give the autocorrelation function the form \(A\langle f^2\rangle e^{-\frac{t}{\tau_r}}\) by assuming all processes are &lt;a href=&quot;https://en.wikipedia.org/wiki/Gaussian_process&quot;&gt;Gaussian&lt;/a&gt; and &lt;a href=&quot;https://en.wikipedia.org/wiki/Markov_process&quot;&gt;Markovian&lt;/a&gt;. The autocorrelation function can then be related to spectral densities using the &lt;a href=&quot;https://en.wikipedia.org/wiki/Wiener%E2%80%93Khinchin_theorem&quot;&gt;Wiener-Khinchin theorem&lt;/a&gt; after which one can derive dynamical things relating to relaxation phenomena, like the famous &lt;a href=&quot;https://en.wikipedia.org/wiki/Fluctuation_dissipation_theorem&quot;&gt;&lt;em&gt;Fluctuation-Dissipation theorem&lt;/em&gt;&lt;/a&gt; for instance. The FD Theorem, first articulated by Einstein, is a statement about how the very &amp;quot;random&amp;quot; forces that cause the fluctuation of a quantity, also results in damping that quantity. For a simple introduction, read the first chapter of &lt;em&gt;Zwanzig, R., 2001. Nonequilibrium statistical mechanics. Oxford University Press, USA.&lt;/em&gt; As tempting as it is to equate time averages with expectation values, it must be kept in mind that in doing so, we are - inadvertently at least - imposing upon Nature non-trivial dynamical behavior that stems from our ignorance of the underlying complexities. Many Physicists seem unphased by Ergodicity. To quote the great &lt;a href=&quot;https://en.wikipedia.org/wiki/Kip_Thorne&quot;&gt;Kip Thorne&lt;/a&gt; from his lecture notes on random processes, where he confines all discussions to ergodic phenomena, &lt;em&gt;&amp;quot;This (ergodic hypothesis), in principle, is a severe restriction. In practice, for a physicist, it is not severe at all. In physics one&amp;#39;s objective, when defining random variables that last forever and when introducing ensembles, is usually to acquire computational techniques for dealing with a single, or a small number of random variables \(y(t)\), studied over finite lengths of time; and one acquires those techniques by defining one&amp;#39;s conceptual infinite-duration random variables and ensembles in such a way that they satisfy the ergodic hypothesis.&amp;quot;&lt;/em&gt; Indeed, this model of reality yields amazingly useful results that have paved much of the way in high precision experimentation. But it has come at the price of mixing probability theory with &amp;quot;stochastic&amp;quot;, &amp;quot;chaotic&amp;quot;, and &amp;quot;ergodic&amp;quot; ideas. The current discussion aims at quantifying the degree of reasonableness of such assumptions.&amp;nbsp;&lt;a href=&quot;#fnref3&quot; rev=&quot;footnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;

&lt;li id=&quot;fn4&quot;&gt;
&lt;p&gt;&lt;em&gt;Away-from-equilibrium&lt;/em&gt; entropy is a baffling concept to me. On the one hand, we can assume &lt;em&gt;partial equilibrium&lt;/em&gt; and equate the perturbed entropy to the equilibrium value corresponding to perturbed \(x_i\)&amp;#39;s. On the other hand, the state of the system (and therefore its supposed microstates) are changing in time and this information is nowhere considered while calculating the entropy using the principles of Statistical Mechanics. This difficulty is hardly ever acknowledged in textbooks where it is often simply postulated that entropy is a function of time.&amp;nbsp;&lt;a href=&quot;#fnref4&quot; rev=&quot;footnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;

&lt;/ol&gt;
&lt;/div&gt;
</content>
 </entry>
 
 <entry>
   <title>A no-nonsense derivation of the Navier-Stokes equations</title>
   <link href="https://vyaas.github.io/2015/11/06/A-No-Nonsense-Derivation-of-the-Navier-Stokes-Equations/"/>
   <updated>2015-11-06T00:00:00-08:00</updated>
   <id>https://vyaas.github.io/2015/11/06/A-No-Nonsense-Derivation-of-the-Navier-Stokes-Equations</id>
   <content type="html">&lt;p&gt;Most of what follows are key arguments expounded by Rutherford Aris, Landau and Lifshitz, and Richard Feynman.&lt;/p&gt;

&lt;p&gt;The roadmap for the derivation is as follows:&lt;/p&gt;

&lt;p&gt;1) Clarify what is meant by &amp;quot;velocity of a fluid at a point in space&amp;quot;.&lt;/p&gt;

&lt;p&gt;2) Derive the Reynolds Transport Theorem.&lt;/p&gt;

&lt;p&gt;3) Develop the form of the stress tensor for a Newtonian Fluid.&lt;/p&gt;

&lt;p&gt;4) Derive a corrseponding transport equation for internal energy.&lt;/p&gt;

&lt;p&gt;Warning: A preliminary understanding - if not appreciation - of vector calculus, including tensors, is required to &lt;em&gt;get&lt;/em&gt; whats about to follow. Here are some relevant chapters from Feynman&amp;#39;s Lectures:&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://www.feynmanlectures.caltech.edu/II_02.html&quot;&gt;Differential Calculus of Vector Fields&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://www.feynmanlectures.caltech.edu/II_03.html&quot;&gt;Vector Integral Calculus&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://www.feynmanlectures.caltech.edu/II_31.html&quot;&gt;Tensors&lt;/a&gt;&lt;/p&gt;

&lt;hr&gt;

&lt;p&gt;We begin with the question, &amp;quot;How does one mathematically describe fluid flow?&amp;quot;. Since we have calculus at our disposal, we could describe the motion of Infinitesimal Fluid Packets (henceforth IFPs) and then integrate the resulting equation of motion to get their trajectories. These trajectories will be a function of space and time. There are two ways we can do this:&lt;/p&gt;

&lt;h1&gt;View-1&lt;/h1&gt;

&lt;p&gt;When looking at a fluid flow, you focus on an IFP and say &amp;quot;This IFP originated from \(\vec{\xi_1}\), this IFP originated from \(\vec{\xi_2}\), this one from \(\vec{\xi_3}\) ...&amp;quot; and so on. At a later time \(t\), you&amp;#39;d say something like &amp;quot;The IFP that originated from \(\vec{\xi_1}\) has moved a distance \(\vec{dr_1}\), the IFP that originated from \(\vec{\xi_2}\) has moved a distance \(\vec{dr_2}\), the IFP that originated at \(\vec{\xi_3}\) has moved a distance \(\vec{dr_3}\) ...&amp;quot; and so on. This way, you&amp;#39;re keeping track of all the particles , albeit individually, and thus describing the fluid flow evolution in time.&lt;/p&gt;

&lt;h1&gt;View-2&lt;/h1&gt;

&lt;p&gt;The other way to do this is to look at the entire fluid flow at once. Take a snapshot of it. Attach to it some convenient spatial coordinate system. Say that at some \(\vec{x_1}\) the fluid velocity is \(\vec{v_1}\), at some \(\vec{x_2}\) the velocity is \(\vec{v_2}\), at some \(\vec{x_3}\) the velocity is \(\vec{v_3}\) and so on. Then at a later time \(t\), you take another snapshot of the same domain, affix the same convenient coordinate system and say something like &amp;quot;at \(\vec{x_1}\) the velocity of the fluid has changed by some \(\vec{dv_1}\), at \(\vec{x_2}\) the velocity of the fluid has changed by some \(\vec{dv_2}\), at \(\vec{x_3}\) the velocity has changed by some \(\vec{dv_3}\).&amp;quot;&lt;/p&gt;

&lt;hr&gt;

&lt;p&gt;View-1 seems to ask &amp;quot;Where is that IFP going?&amp;quot; while View-2 asks &amp;quot;How is this picture of the flow going to change?&amp;quot;&lt;sup id=&quot;fnref1&quot;&gt;&lt;a href=&quot;#fn1&quot; rel=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt;. When studying the dynamics of rigid bodies, we always followed particles around. \(\vec{F}=m\vec{a}\) was referring to the acceleration of a particle (View-1), not the position in space at which the velocity was changing (View-2), which is nonsense because the particle can only occupy one point in space (or at most a finite portion of it) at any given time. What velocity and acceleration could we possibly ascribe to empty points in space?! But for fluids, or more generally for continuous media, we don&amp;#39;t run into this problem. In our domain of interest, every point in space is &lt;em&gt;occupied&lt;/em&gt; so every one of these points can be ascribed properties like velocities, accelerations, densities, pressures, temperatures, entropies, concentrations, vorticities, etc. &amp;quot;But how can a point, the teeniest-tinyest imaginable point, have any mass?!&amp;quot; you demand. Indeed, when considering a point in space, chances are that it is occupied more by vacuum than anything else. The points we&amp;#39;re considering in the case of fluids are points which are fairly large. Large enough to comprise \(10^{15}\) molecules and upwards. Large enough to have a mass per unit volume. Large enough to have a mean temperature. Large enough to have molecules colliding with one another to exert pressure on any area upon which they impinge. If your points are any smaller than this, you are guaranteed nonsense, atleast if you choose to use the framework we&amp;#39;re developing. You would be describing regions in space molecules may or may not have arrived at. What we&amp;#39;re doing in fluid mechanics is zooming out so that everything looks nice and continuous and not bumpy and discrete. And since we&amp;#39;re looking to transform from points in space (View-2) to following IFPs (View-1), our IFPs are also to be thought of as being large enough to contain \(10^{15}\) particles, large enough to have a mass per unit volume and so on. Of course, we don&amp;#39;t want our points (or IFPs) to be too large either; we&amp;#39;d lose the resolution we desire to examine important phenomena like diffusion! Similar arguments apply to time. We are describing flow over time-scales much larger than the time-scales of thermal equilibration. Only then can we ascribe to our fluid various thermodynamic properties. Probing into timescales shorter than this leads, once again, to absurdities&lt;sup id=&quot;fnref2&quot;&gt;&lt;a href=&quot;#fn2&quot; rel=&quot;footnote&quot;&gt;2&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;

&lt;p&gt;View-1 and View-2 are equivalent. The laws of Fluid Physics should not change depending on which description you choose. But we need both views. View-2 is convenient for calculations, but View-1 is what we&amp;#39;ve been trained in since high school. So we should learn how to transform all relevant physical quantities from one view to another.&lt;/p&gt;

&lt;p&gt;View-1: If \(\vec{\xi}\) is the initial position of an IFP, the property \(\psi\) of the IFP can be denoted as&lt;/p&gt;

&lt;p&gt;$$ \psi = \psi(\vec{\xi},t) $$&lt;/p&gt;

&lt;p&gt;Each IFP originates at some \(\vec{\xi_i}\) where \(i\) can take values from 1 to \(\infty\)! That&amp;#39;s what our continuum approximation entails. Finite domain; Infinite number of IFPs. That&amp;#39;s where the I in IFP comes from. Since the initial position of any particle is a continuous function of space, we can write&lt;/p&gt;

&lt;p&gt;$$ \vec{\xi} = \vec{\xi}(\vec{x},t) $$&lt;/p&gt;

&lt;p&gt;This simply says that all initial positions of IFPs belong somewhere. Remember that functions are always answers to questions. Here the question goes something like &amp;quot;which IFP is located at position \(\vec{x}\) and time \(t\)?&amp;quot;. Answer: &amp;quot;\(\vec{\xi}\)!&amp;quot; You must think of \(\vec{\xi}\) as taking on an infinite number of values! That isn&amp;#39;t a problem as long as we can associate each of these values to a position in space. Analytically, this means the transformation matrix (also called the Jacobian matrix \(\tilde{J}\)) is non-singular everywhere:&lt;/p&gt;

&lt;p&gt;$$ \vec{dx} = \tilde{J}\vec{d\xi} $$&lt;/p&gt;

&lt;p&gt;This matrix \(\tilde{J}\) maps every initial position of an ISP to every point on the domain uniquely.&lt;/p&gt;

&lt;p&gt;View-2: Every point in space is occupied by some IFP. Therefore every point in space can be ascribed a property \(\psi\) such that&lt;/p&gt;

&lt;p&gt;$$ \psi = \psi(\vec{x},t) $$&lt;/p&gt;

&lt;p&gt;So \(\psi\) is a function of space and time! At a given point in space and time, this \(\psi\) had better be equal to the \(\psi\) we would get in View-1, i.e. the value of a property at some point is equal to the value of the property of the IFP at that point!&lt;/p&gt;

&lt;p&gt;$$ \psi(\vec{x},t) = \psi[\vec{\xi}(\vec{x},t),t] $$&lt;/p&gt;

&lt;p&gt;The converse is equally true:&lt;/p&gt;

&lt;p&gt;$$ \psi(\vec{\xi},t) = \psi[\vec{x}(\vec{\xi},t),t] $$&lt;/p&gt;

&lt;p&gt;We&amp;#39;re finally in a position to say what we mean by a time derivative. Let&amp;#39;s reserve unique symbols for each case first: Let \(\frac{d}{dt}\) be the time derivative of a quanitity keeping \(\vec{\xi}\) constant. Let \(\frac{\partial}{\partial t}\) be the time derivative of a quanitity keeping \(\vec{x}\) constant.&lt;/p&gt;

&lt;p&gt;Behold, the velocity of an IFP:&lt;/p&gt;

&lt;p&gt;$$ \vec{u} = \frac{d\vec{x}}{dt} $$&lt;/p&gt;

&lt;p&gt;It&amp;#39;s worth repeating what this means! By keeping \(\vec{\xi}\) constant while taking the derivative of the location \(\vec{x}\) of the IFP with respect to time, we&amp;#39;re making sure that we&amp;#39;re tracking that IFP and that IFP alone!&lt;/p&gt;

&lt;p&gt;Now, since we know
$$ \psi(\vec{\xi},t) = \psi[\vec{x}(\vec{\xi},t),t] $$
to be true, we proceed taking time derivatives as follows:
$$ \frac{d\psi}{dt} = \frac{d\vec{x}}{dt}\cdot\nabla\psi+ \frac{\partial\psi}{\partial t} $$
$$ \frac{d\psi}{dt} = \vec{u}\cdot\nabla\psi+ \frac{\partial\psi}{\partial t} $$&lt;/p&gt;

&lt;p&gt;This was the transformation rule we&amp;#39;ve been trying to articulate. This is just what we need to derive a very important theorem due to Reynolds. Note that \(\nabla\) is the gradient operator. \(\nabla\psi\) is a vector and each component is its derivative in its corresponding direction. &lt;/p&gt;

&lt;p&gt;Onward.&lt;/p&gt;

&lt;hr&gt;

&lt;h1&gt;The Transport Theorem of Reynolds&lt;/h1&gt;

&lt;p&gt;Imagine an IFP located initially at some \(\vec{\xi}\). Let&amp;#39;s say this IFP moves to some location \(\vec{x}\) over the course of some small time \(dt\). This IFP&amp;#39;s volume, initially \(V_0\), is given by the cross-product \(|d\vec{\xi_1}\times d\vec{\xi_2}\times d\vec{\xi_3}|\). At a later time, this IFP&amp;#39;s volume could have changed owing to some compression, expansion or concentration change. This volume \(dV\) is given by \(|d\vec{x_1}\times d\vec{x_2}\times d\vec{x_3}|\). We&amp;#39;ve already seen that &lt;/p&gt;

&lt;p&gt;$$ \vec{d\xi} = \tilde{J}\vec{dx} $$&lt;/p&gt;

&lt;p&gt;Therefore the relationship between the old and new volumes is&lt;/p&gt;

&lt;p&gt;$$ dV = |\tilde{J}|dV_0 $$
where \(|\tilde{J}|\) is the determinant of the Jacobian Matrix. &lt;/p&gt;

&lt;p&gt;&lt;center&gt;&lt;img src = &quot;https://vyaas.github.io/public/images/ifp_deformation.png&quot;\&gt;&lt;/center&gt;&lt;/p&gt;

&lt;p&gt;Now, how does this ratio of volumes change in time; what is \(\frac{d|\tilde{J}|}{dt}\)?&lt;/p&gt;

&lt;p&gt;The determinant \(|\tilde{J}|\) looks like this:
$$ \begin{vmatrix}
\frac{\partial x_1}{\partial \xi_1}  &amp;amp; \frac{\partial x_1}{\partial \xi_2} &amp;amp; \frac{\partial x_1}{\partial \xi_3} \\ 
\frac{\partial x_2}{\partial \xi_1}  &amp;amp; \frac{\partial x_2}{\partial \xi_2} &amp;amp; \frac{\partial x_2}{\partial \xi_3} \\
\frac{\partial x_3}{\partial \xi_1}  &amp;amp; \frac{\partial x_3}{\partial \xi_2} &amp;amp; \frac{\partial x_3}{\partial \xi_3}
\end{vmatrix} $$&lt;/p&gt;

&lt;p&gt;While taking its time derivative, keep two things in mind:&lt;/p&gt;

&lt;p&gt;1) $$ \frac{d}{dt}\frac{\partial x_i}{\partial\xi_i} = \frac{\partial}{\partial \xi_i}\frac{dx_i}{dt} = \frac{\partial u_i}{\partial\xi_i} $$
We were able to interchange the derivatives because \(\frac{d}{dt}\) keeps \(\vec{\xi}\) constant.&lt;/p&gt;

&lt;p&gt;2) Since \(\vec{u}\) is a function of \(\vec{x}\), we can write
$$ \frac{\partial u_i}{\partial\xi_j} = \frac{\partial u_i}{\partial x_1}\frac{\partial x_1}{\partial\xi_j} +
                      \frac{\partial u_i}{\partial x_2}\frac{\partial x_2}{\partial\xi_j} +
                      \frac{\partial u_i}{\partial x_3}\frac{\partial x_3}{\partial\xi_j} $$&lt;/p&gt;

&lt;p&gt;We then get&lt;/p&gt;

&lt;p&gt;$$ \frac{d|\tilde{J}|}{dt} = |\tilde{J}|\nabla\cdot\vec{u} $$
or
$$ \nabla\cdot\vec{u} = \frac{dln(|(\tilde{J}|)}{dt} $$&lt;/p&gt;

&lt;p&gt;Hence, we find that the divergence of the velocity tells us how much an IFP is changing volume. When it is zero, we say the flow is incompressible. &lt;/p&gt;

&lt;p&gt;Now we&amp;#39;re ready to derive the indispensable Reynolds Transport Theorem. We wish to find the time derivative of an integral this time. Namely&lt;/p&gt;

&lt;p&gt;$$ \Psi(t) = \oint_{V(t)} \psi(\vec{x},t) dV $$&lt;/p&gt;

&lt;p&gt;Here, we&amp;#39;ve integrated over a bunch of IFPs whose volumes are allowed to change in time. How are we to take the time derivative \(\frac{d}{dt}\) (keeping \(\vec{\xi}\) constant) of this? We&amp;#39;re not allowed to bring the derivative inside the integral since \(dV\) itself changes with time. But, as luck may have it...&lt;/p&gt;

&lt;p&gt;$$ \frac{d\Psi}{dt} = \frac{d}{dt}\oint_{V(t)}\psi(\vec{x},t) dV $$
$$             = \frac{d}{dt}\oint_{V(t)}\psi |\tilde{J}|dV_0 $$
$$             = \oint_{V(t)}\frac{d}{dt} (\psi |\tilde{J}|) dV_0 $$
$$             = \oint_{V(t)}\left(|\tilde{J}|\frac{d}{dt}\psi + \psi\frac{d}{dt}|\tilde{J}|\right)dV_0 $$
$$             = \oint_{V(t)}\left(\frac{d}{dt}\psi + \psi\nabla\cdot\vec{u}\right)|\tilde{J}|dV_0 $$
$$             = \oint_{V(t)}\left(\vec{u}\cdot\nabla\psi+ \frac{\partial\psi}{\partial t} + \psi\nabla\cdot\vec{u}\right)|\tilde{J}|dV_0 $$
$$             = \oint_{V(t)}\left(\frac{\partial\psi}{\partial t} + \nabla\cdot\psi\vec{u}\right)dV $$&lt;/p&gt;

&lt;p&gt;This result is useful as it stands, but we can do better and express this in terms of fluxes. Apply Green&amp;#39;s Theorem on the divergence term to get:&lt;/p&gt;

&lt;p&gt;$$ \frac{d}{dt}\oint_{V(t)} \psi(\vec{x},t) dV  = \oint_{V(t)} \frac{\partial\psi}{\partial t} dV + \oint_{S(t)}\psi\vec{u}\cdot\hat{n}dS $$&lt;/p&gt;

&lt;p&gt;There it is. And like most things simple, it is very powerful. In words, it says that the rate of change of the volume integral of a quantity (i.e. an integral over some continuum of IFPs), is equal to the volume integral of how quickly that quantity is changing in time and the surface integral of the rate at which the quanitity is entering said IFPs (we call this &lt;em&gt;flux&lt;/em&gt;). Here \(S(t)\) is the instantaneous surface area of the volume considered and \(\hat{n}\) is a local unit normal vector to the infinitesimal surface element \(dS\). Having integral relationships between quanitites is useful because they&amp;#39;re more generally applicable. Sometimes, the local details of a phenomenon might remain elusive; for instance, the events in a shock wave. Even if our continuum assumptions, i.e. our differential equations break down at the shock wave, our integral relations will still hold!&lt;/p&gt;

&lt;p&gt;As a first application of the theorem, we can derive the conservation of mass. If \(\rho\) is the density of an IFP,&lt;/p&gt;

&lt;p&gt;$$ \frac{d}{dt}\oint_{V} \rho(\vec{x},t) dV  = \oint_{V} \frac{\partial\rho}{\partial t} dV + \oint_{S(t)}\rho\vec{u}\cdot\hat{n}dS = 0$$
$$             \oint_{V}\left(\frac{\partial\rho}{\partial t} + \nabla\cdot\rho\vec{u}\right)dV  = 0 $$&lt;/p&gt;

&lt;p&gt;If an integral is necessarily zero everywhere, it is sufficient if its integrand is zero everywhere as well. Therefore,
$$             \frac{\partial\rho}{\partial t} + \nabla\cdot\rho\vec{u}  = 0 $$&lt;/p&gt;

&lt;p&gt;The Reynolds Transport Theorem has set us on the right track. We will henceforth think of rates of change exclusively in terms of fluxes and volume integrals. If we feel the need for a differential equation, we&amp;#39;ll apply Green&amp;#39;s theorem to convert fluxes to divergences.&lt;/p&gt;

&lt;hr&gt;

&lt;h1&gt;The Stress Tensor&lt;/h1&gt;

&lt;p&gt;For a continuum of IFPs, we can apply Newton&amp;#39;s second law like so:&lt;/p&gt;

&lt;p&gt;Rate of change of momentum = Body Forces + Surface Forces&lt;/p&gt;

&lt;p&gt;Body forces are volumetric things and act on the mass of the fluid. Like gravity, or if the fluid is charged, electric fields. We always define such forces per unit volume. Surface forces act on surfaces and we define them per unit area. We call them stresses. Their origin is in the actions of molecules (duh, everything is!). Pressure for instance is the rate of momentum transferred to a unit area via molecular collisions in a gas. Shear stress, microscopically, is fast molecules transferring momentum to slow molecules via molecular collisions. Whenever an IFP deforms, mechanical forces (pressure) and thermal forces (viscosity) tend to &amp;quot;reform&amp;quot; the IFP, i.e. restore it to equilibrium. The equation of words written above can be written symbolically as:&lt;/p&gt;

&lt;p&gt;$$ \frac{d}{dt}\oint_{V(t)} \rho\vec{u} dV  = \oint_{V(t)} \rho\vec{f} dV + \oint_{S(t)}\vec{t_{(n)}}dS $$&lt;/p&gt;

&lt;p&gt;This is not Reynolds&amp;#39; Transport Theorem! This is just Newton&amp;#39;s second law! Something important emerges immediately when it is written this way. If you shrink the volume of the continuum over which the integrals are taken down to that of an IFP, the volume integrals go to zero faster than the surface integral because volume scales with the length cubed whereas area scales with the length &lt;em&gt;squared&lt;/em&gt;. This means that the surface forces are &lt;em&gt;always&lt;/em&gt; in local equilibrium for an IFP. This is another way to say that our idealized IFPs correspond to a volume of fluid whose size is so small, that the forces acting on its surface are in mechanical equilibrium all the time. We can make use of this fact in deriving the so called &lt;em&gt;stress tensor&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;For the time-being, imagine a 2-dimensional square shaped IFP (because it&amp;#39;s easier to draw). If we cut this square across the diagonal, we have a nice right angled triangle with three edges. There is a force acting on each of these edges, but they must all add up to zero. If \(dl\) is the length of the hypotenuse, and \(dl_1\) and \(dl_2\) are the lengths of the perpendicular edges, and the force per unit length or &lt;em&gt;length&lt;/em&gt; stress is \(vec{t}\),&lt;/p&gt;

&lt;p&gt;$$ \vec{t_{(n)}}dl - \vec{t_1}dl_1 - \vec{t_2}dl_2  = 0 $$&lt;/p&gt;

&lt;p&gt;&lt;center&gt;&lt;img src = &quot;https://vyaas.github.io/public/images/triangle_vectors.png&quot;\&gt;&lt;/center&gt;&lt;/p&gt;

&lt;p&gt;Here, the signs of the forces are an artefact of the coordinate system. You should get the right signs as long as you&amp;#39;re consistent with the directions of your forces relative to the surface normals. Here, all forces are pointing outside the triangle. To be more precise, since the ratios of the the length of a perpendicular to the hypotenuse is a component of the normal vector to the hypotenuse, we can write &lt;/p&gt;

&lt;p&gt;$$ \vec{t_{(n)}} = \vec{t_1}|n_1| + \vec{t_2}|n_2|  $$&lt;/p&gt;

&lt;p&gt;Remember that \(\vec{t_1}\) and \(\vec{t_2}\) are vectors which are, in general,  allowed to point in any direction (we&amp;#39;ve only constrained what their sum must be). It would make sense to resolve the components of these vectors in the \(\hat{n_1}\) and \(\hat{n_2}\) directions. If \(t_{n_1}\) and \(t_{n_2}\) are the components of the left hand side force in these directions, we can write:&lt;/p&gt;

&lt;p&gt;$$ \begin{pmatrix}
t_{n_1} \\
t_{n_2}
\end{pmatrix}  = 
\begin{pmatrix}
T_{11} &amp;amp; T_{12} \\
T_{21} &amp;amp; T_{22} \\
\end{pmatrix} 
\begin{pmatrix}
n_1 \\
n_2
\end{pmatrix} 
$$&lt;/p&gt;

&lt;p&gt;The right hand side is the dot product between a tensor and a vector so we can write the above more compactly as&lt;/p&gt;

&lt;p&gt;$$ \vec{t_{(n)}} = \tilde{T}\cdot\vec{n} $$&lt;/p&gt;

&lt;p&gt;or - even better - in index notation&lt;sup id=&quot;fnref3&quot;&gt;&lt;a href=&quot;#fn3&quot; rel=&quot;footnote&quot;&gt;3&lt;/a&gt;&lt;/sup&gt; as&lt;/p&gt;

&lt;p&gt;$$ t_i = T_{ij}n_j $$&lt;/p&gt;

&lt;p&gt;We can forget about the triangle we cut from our 2d square shaped IFP now! Everything we&amp;#39;ve written is coordinate system independent! The left hand side comprises the components of an arbitrary stress over an arbitrary area of an IFP. The right hand side is a linear combination of the components of the normal vector to this area. The weights appearing in this linear combination are the components of a tensor. All this generalizes easily to the three dimensional case, where \(\tilde{T}\) becomes a nine-component tensor. This makes sense; in order to express the components of a force acting per unit area of an IFP (cube shaped), and keeping in mind that that force must be related to the forces over the other areas of the cube because the IFP must be in mechanical equilibrium, the total number of components in the tensor is nine (the cube has nine surfaces).&lt;/p&gt;

&lt;p&gt;$$ \begin{pmatrix}
t_{n_1} \\
t_{n_2} \\
t_{n_3}
\end{pmatrix}  = 
\begin{pmatrix}
T_{11} &amp;amp; T_{12}  &amp;amp; T_{13} \\
T_{21} &amp;amp; T_{22}  &amp;amp; T_{23} \\
T_{31} &amp;amp; T_{32}  &amp;amp; T_{33} \\
\end{pmatrix} 
\begin{pmatrix}
n_1 \\
n_2 \\
n_3
\end{pmatrix} 
$$&lt;/p&gt;

&lt;p&gt;There is a compelling reason for why this stress tensor needs to be symmetric. Consider any two perpendicular surfaces of our IFP, say surfaces with normals \(\hat{n_1}\) and \(\hat{n_2}\). Then, the forces on the two surfaces are respectively&lt;/p&gt;

&lt;p&gt;$$ T_{11}\hat{n_1} + T_{12}\hat{n_2} + T_{13}\hat{n_3} $$
$$ T_{21}\hat{n_1} + T_{22}\hat{n_2} + T_{23}\hat{n_3} $$&lt;/p&gt;

&lt;p&gt;If \(T_{12}\) is not equal to \(T_{21}\), our IFP should begin to spin due to the torque set up by these imbalanced forces. When we stated that the sum of the forces had to equal zero to assure mechanical equilibrium, we were being only partially correct; the sum of the &lt;em&gt;torques&lt;/em&gt; need to equal zero as well! If not, our IFPs would all be spinning, infinitely fast in fact, because if even the slightest torque were set up, the moment of inertia, being zero for an IFP, would be compensated for by an infinite angular acceleration. Since we&amp;#39;re staying clear of such ridiculousness, we must insist that the torque set up by the surface forces on the IFP is zero &lt;sup id=&quot;fnref4&quot;&gt;&lt;a href=&quot;#fn4&quot; rel=&quot;footnote&quot;&gt;4&lt;/a&gt;&lt;/sup&gt;. This information is contained in the fact that the stress tensor is symmetric. This does not mean that our IFP cannot have a vorticity which is defined as \(\nabla\times\vec{u}\). Newton&amp;#39;s second law for our IFP in terms of the stress tensor looks like:&lt;/p&gt;

&lt;p&gt;$$ \frac{d}{dt}\oint_{V(t)} \rho\vec{u} dV  = \oint_{V(t)} \rho\vec{f} dV + \oint_{S(t)}\tilde{T}\cdot\hat{n}dS $$&lt;/p&gt;

&lt;p&gt;Using the Reynolds Transport Theorem, it becomes&lt;/p&gt;

&lt;p&gt;$$ \oint_{V(t)} \frac{\partial(\rho\vec{u})}{\partial t}dV  + \oint_{S(t)}\rho\vec{u}\otimes\vec{u}\cdot\hat{n}dS = \oint_{V(t)} \rho\vec{f} dV + \oint_{V(t)}\nabla\cdot\tilde{T}dV $$&lt;/p&gt;

&lt;p&gt;Upon applying Green&amp;#39;s Theorem,&lt;/p&gt;

&lt;p&gt;$$ \oint_{V(t)} \left(\frac{\partial}{\partial t}\rho\vec{u} + \nabla\cdot(\rho\vec{u}\otimes\vec{u})\right) dV  = \oint_{V(t)} \rho\vec{f} dV + \oint_{V(t)}\nabla\cdot\tilde{T}dV $$&lt;/p&gt;

&lt;p&gt;Which means,&lt;/p&gt;

&lt;p&gt;$$ \frac{\partial}{\partial t}\rho\vec{u} + \nabla\cdot(\rho\vec{u}\otimes\vec{u}) = \rho\vec{f} + \nabla\cdot\tilde{T} $$&lt;/p&gt;

&lt;p&gt;We need to express the components of the stress tensor in terms of the velocity field to close the problem. The concepts are directly inspired by studies in solid mechanics. There, a solid experiences stress only if it is strained, i.e. somehow deformed from its equilibrium configuration. In fluid mechanics, an IFP is strained when different portions of it are moving at different velocities, thus causing deformation. We accounted for deformation already by insisting that \(dV = |\tilde{J}|dV_0\). But this deformation will set up internal stress in the fluid; IFPs will feel surface forces due to this deformation. Let&amp;#39;s first consider the simple case in which there is no velocity, i.e. the fluid is static. In that case, whatever \(\nabla\cdot\tilde{T}\) is, it is balanced by the body forces:&lt;/p&gt;

&lt;p&gt;$$ \nabla\cdot\tilde{T} = -\rho\vec{f} $$&lt;/p&gt;

&lt;p&gt;The stress tensor has to be isotropic, i.e. its components must not change if we rotate our coordinate system. The components of the body forces certainly change for they have directionality associated with them. The stress tensor however doesn&amp;#39;t because all it is allowed to depend on are the velocities, which are zero everywhere! So it ought to look like this:&lt;/p&gt;

&lt;p&gt;$$ T_{ij} = -P\delta_{ij} $$&lt;/p&gt;

&lt;p&gt;\(\delta_{ij}\) is trivially isotropic. In matrix notation, it is the identity matrix:&lt;/p&gt;

&lt;p&gt;$$ \tilde{\delta} = 
\begin{pmatrix}
1 &amp;amp; 0 &amp;amp; 0 \\
0 &amp;amp; 1 &amp;amp; 0 \\
0 &amp;amp; 0 &amp;amp; 1 \\
\end{pmatrix}
$$&lt;/p&gt;

&lt;p&gt;\(P\) is what we call &lt;em&gt;pressure&lt;/em&gt; and it is a scalar. The minus sign we&amp;#39;ve stuck in front of it is only a convenience. Therefore, our force law for hydrostatics becomes:&lt;/p&gt;

&lt;p&gt;$$ \nabla P = \rho\vec{f} $$&lt;/p&gt;

&lt;p&gt;If \(\vec{f}\) is the acceleration due to gravity \(\vec{g}\), we get the familiar looking relationship between pressure and height. Assuming our coordinate system&amp;#39;s z axis lines up with the direction of \(\vec{g}\), we integrate once to get&lt;/p&gt;

&lt;p&gt;$$ P = P_{0} + \rho g h $$&lt;/p&gt;

&lt;p&gt;What about the case in which the velocities are non-zero? Here we make the following hypotheses:&lt;/p&gt;

&lt;p&gt;1) The stress tensor should be modified by adding to another tensor to the one derived above. We do this so that in the limiting case of no velocities, we get the right expression back. So,&lt;/p&gt;

&lt;p&gt;$$ T_{ij} = -P\delta_{ij} + \sigma_{ij} $$&lt;/p&gt;

&lt;p&gt;2) \(\sigma_{ij}\) must depend on first derivatives of velocity. This is a statement in phenomenology. Since the deformation of IFPs is caused by velocity gradients, it is quite reasonable to express the stresses as being proportional to these. There are parallels beyond those in solid mechanics. In the subject of heat transfer, we learn that the energy flux is proportional to the gradient of the temperature. In mass transfer, the species flux is proportional to the gradient of the concentration. If we keep with this linear themed phenomenology, we will require the momentum flux to be proportional to the velocity gradient as well.&lt;/p&gt;

&lt;p&gt;3) The dependence on velocity gradients cannot be arbitrary but must occur as exactly two possible combinations: \(\frac{\partial u_{i} }{\partial x_{j} } + \frac{\partial u_{j}}{\partial x_{i}}\) and \(\frac{\partial u_l}{\partial x_l}\). Let&amp;#39;s discuss the sum first. Consider a 2-d plane in which an IFP is moving. Suppose it is rotating about some axis such that its speed is proportional to its distance from the axis \(\vec{r}\), we can say that it is rotating like a rigid body. The condition \(\vec{u}=\vec{r}\times\vec{\omega}\) is derived specifically under the constraint that no part of the body deforms. Hence the descriptive &amp;quot;rigid&amp;quot;. So if our IFP is undergoing rigid body rotation, it must not deform. If it does not deform, it is not stressed. The combination \(\frac{\partial u_{i} }{\partial x_{j} } + \frac{\partial u_{j}}{\partial x_{i}}\) reduces to zero in just such a case.  The second combination \(\frac{\partial u_i}{\partial x_i}\) is the only other possible combination left. Therefore,&lt;/p&gt;

&lt;p&gt;$$ \sigma_{ij} = a \left(\frac{\partial u_{i} }{\partial x_{j} } + \frac{\partial u_{j}}{\partial x_{i}}\right) + b\frac{\partial u_l}{\partial x_l}\delta{ij} $$&lt;/p&gt;

&lt;p&gt;4) To figure out what how a and b are, we invoke a rather controversial hypothesis made by Stokes. The components of the stress tensor \(T_{11}\), \(T_{22}\), and \(T_{33}\) are quite significant. The trace, which is the sum of these three components, is an invariant of the tensor, meaning that under any coordinate system rotation, the trace of the transformed tensor will remain unchanged. The trace is denoted \(T_{ii}\). The mean stress is therefore given by&lt;/p&gt;

&lt;p&gt;$$ \langle T\rangle  = \frac{1}{3}T_{ii} $$
$$        = -P + \frac{1}{3}\sigma_{ii} $$
$$   = -P + \frac{1}{3}\left(2a\frac{\partial u_i}{\partial x_i} + 3b\frac{\partial u_l}{\partial x_l}\right) $$
$$   = -P +\left(\frac{2}{3}a + 3b\right)\frac{\partial u_l}{\partial x_l} $$&lt;/p&gt;

&lt;p&gt;Stokes hypothesized that the mean stress experienced by an IFP must only be proportional to \(P\) and nothing else. He is saying that the average force felt by any unit area of an IFP is solely due to the local thermodynamic pressure. In such a case,&lt;/p&gt;

&lt;p&gt;$$ \frac{2}{3}a + b = 0 $$
or
$$ b = -\frac{2}{3}a $$&lt;/p&gt;

&lt;p&gt;We replace &amp;#39;\(a\)&amp;#39; with \(\mu\) and call it the &lt;em&gt;dynamic viscosity&lt;/em&gt;. It must always be positive to guarantee entropy increase. Thus&lt;/p&gt;

&lt;p&gt;$$ \sigma_{ij} = \mu \left(\frac{\partial u_{i} }{\partial x_{j} } + \frac{\partial u_{j}}{\partial x_{i}} -\frac{2}{3}\frac{\partial u_l}{\partial x_l}\delta_{ij} \right)$$&lt;/p&gt;

&lt;p&gt;It has been shown in many experiments that the hypothesis of Stokes breaks down when acoustic phenomena like sound absorption, or the high velocity divergence that accompanies hypersonic flows are significant. We therefore remedy this by correcting the hypothesis like so&lt;/p&gt;

&lt;p&gt;$$ \frac{2}{3}\mu + b = \kappa $$
or
$$ b = \kappa - \frac{2}{3}\mu$$&lt;/p&gt;

&lt;p&gt;This \(\kappa\) is called the bulk viscosity. It too is confined to positive values for the sake of the second law of Thermodynamics.&lt;/p&gt;

&lt;hr&gt;

&lt;p&gt;We can now write the full Navier-Stokes Equations. In index notation they are written as,&lt;/p&gt;

&lt;p&gt;$$ \frac{\partial(\rho u_i)}{\partial t} + \frac{\partial(\rho u_i u_j)}{\partial x_j} = \rho f_i + \frac{\partial (T_{ij})}{\partial x_j} $$
where
$$ T_{ij} = -P\delta_{ij} + \mu \left(\frac{\partial u_{i} }{\partial x_{j} } + \frac{\partial u_{j}}{\partial x_{i}} -\frac{2}{3}\frac{\partial u_l}{\partial x_l}\delta_{ij} \right) + \kappa\frac{\partial u_l}{\partial x_l}\delta_{ij} $$&lt;/p&gt;

&lt;p&gt;In vector notation,&lt;/p&gt;

&lt;p&gt;$$ \frac{\partial}{\partial t}\rho\vec{u} + \nabla\cdot(\rho\vec{u}\otimes\vec{u}) = \rho\vec{f} + \nabla\cdot\tilde{T} $$
where&lt;/p&gt;

&lt;p&gt;$$ \tilde{T} = -P\tilde{\delta} + \mu\left((\nabla\vec{u}) + (\nabla\vec{u})^T - \frac{2}{3}\nabla\cdot\vec{u}\right) + \kappa\nabla\cdot\vec{u} $$&lt;/p&gt;

&lt;p&gt;There we have it! There is much to comment on these equations, but I&amp;#39;ll leave that for another post. &lt;/p&gt;

&lt;p&gt;I&amp;#39;ve heard many call the NS equations ugly. I think they&amp;#39;re particularly beautiful. Look at all the considerations that had to be made so we could mathematically articulate fluid flow! Isn&amp;#39;t the result inspiring? It sure is to me! &lt;/p&gt;

&lt;hr&gt;

&lt;h1&gt;The Transport of Internal Energy&lt;/h1&gt;

&lt;p&gt;Let us evaluate the rate of work done on the IFP by the various stresses , i.e. \(\oint_{S}\ v_iT_{ij}n_jdS\) (or \(\oint_{S}\vec{v}\cdot\vec{t}_{(n)}dS\)).&lt;/p&gt;

&lt;p&gt;$$ \oint_{S}\ v_iT_{ij}n_jdS = \oint_{V}\nabla\cdot(\vec{v}\tilde{T})dV $$
$$  = \oint_{V}(\vec{v}\cdot(\nabla\cdot\tilde{T}) + \tilde{T}:\nabla\vec{v})dV $$&lt;/p&gt;

&lt;p&gt;The second term is what is called a double contraction; it produces a scalars, as it should. The first term is the scalar product of the velocity and the divergence of the stress tensor. But the divergence of the stress tensor already appears in the Navier Stokes equation, so we can just pull it from there:&lt;/p&gt;

&lt;p&gt;$$  \oint_{S}\ v_iT_{ij}n_jdS  $$
$$  = \oint_{V}(\vec{v}\cdot(\rho\frac{d\vec{v}}{dt}-\rho f) + \tilde{T}:\nabla\vec{v})dV $$&lt;/p&gt;

&lt;p&gt;Rearrange this to arrive at the rate of change of kinetic energy for an IFP:&lt;/p&gt;

&lt;p&gt;$$ \frac{d}{dt}\oint_{V}\frac{1}{2}\rho v^2dV = \oint_{V}\rho \vec{f}\cdot\vec{v}dV - \oint_{V}\tilde{T}:\nabla{v}dV + \oint_{S}\vec{v}\cdot\vec{t}_{(n)}dS $$&lt;/p&gt;

&lt;p&gt;(We note that 
$$ \oint_{V}\vec{v}\cdot(\rho\frac{d\vec{v}}{dt})dV $$
$$  = \oint_{V}\frac{\rho}{2}(\frac{d v^2}{dt})dV $$
$$  = \oint_{V}\frac{d}{dt}(\frac{\rho v^2}{2})dV  - \oint_{V}\frac{v^2}{2}\frac{d\rho}{dt}dV $$
$$  = \frac{d}{dt}\oint_{V}\frac{\rho v^2}{2}dV $$
which all follows from the same tricks we used in deriving Reynolds&amp;#39; Transport Theorem.)&lt;/p&gt;

&lt;p&gt;Upon examining the rate of change of kinetic energy, the first term on the right hand side is the power expended by body forces, the second term denotes the power expended by internal stresses and the third term denotes the power expended by surface stresses. We can decompose the second term into power expended by &lt;em&gt;reversible&lt;/em&gt; internal stresses and power expended by &lt;em&gt;irreversible&lt;/em&gt; internal stresses:&lt;/p&gt;

&lt;p&gt;$$ \tilde{T}:\vec{v} = -P\nabla\cdot\vec{v} + \Phi $$&lt;/p&gt;

&lt;p&gt;The first term on the right hand side is the rate of change of strain energy and the second is viscous dissipation. It is this dissipation that leads to an increase in entropy of the IFP.&lt;/p&gt;

&lt;p&gt;We began by expressing the surface stresses in terms of the kinetic energy of the IFP. But what about the total energy of the IFP? The rate of change of the total energy of the IFP (the sum of internal and kinetic energies) must be equal to the sum of power due to the body forces, power due to the surface stresses and the heat loss from the IFP due to conduction, radiation, mass diffusion, etc. We can write this integrally as&lt;/p&gt;

&lt;p&gt;$$ \frac{d}{dt}\oint_{V}\rho(\frac{1}{2}v^2 + \epsilon)dV = \oint_{V}\rho \vec{f}\cdot\vec{v}dV + \oint_S\vec{v}\cdot\vec{t}_{(n)}dS - \oint_S\vec{q}\cdot\hat{n}dS $$&lt;/p&gt;

&lt;p&gt;Here, \(\epsilon\) is the specific internal energy of the IFP (energy per unit mass) and \(\vec{q}\) is the heat flux vector. Subtract the rate of change of kinetic energy from this, rearrange ( we should now be experts in moving time derivatives into and out of integrals), apply Green&amp;#39;s theorem to convert surface integrals to volume integrals to finally get:&lt;/p&gt;

&lt;p&gt;$$ \rho\frac{d\epsilon}{dt} = \nabla\cdot\vec{q} - P(\nabla\cdot\vec{v}) + \Phi $$&lt;/p&gt;

&lt;p&gt;So the rate of change of internal energy of the IFP is simply given by how much heat enters it, how much pressure work it exerts on the surrounding IFPs, and the viscous dissipation. Since the dissipation is always positive, we lose kinetic energy while gaining it in the form of internal (thermal) energy.&lt;/p&gt;

&lt;h1&gt;Footnotes&lt;/h1&gt;

&lt;div class=&quot;footnotes&quot;&gt;
&lt;hr&gt;
&lt;ol&gt;

&lt;li id=&quot;fn1&quot;&gt;
&lt;p&gt;View-1 is popularly known as the &amp;quot;Lagrangian&amp;quot; view point while View-2 is called &amp;quot;Eulerian&amp;quot;. These names obstruct understanding, which is why I have avoided using them. No disrespect intended of course.&amp;nbsp;&lt;a href=&quot;#fnref1&quot; rev=&quot;footnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;

&lt;li id=&quot;fn2&quot;&gt;
&lt;p&gt;Such arguments are formally made in the subject of statistical mechanics where the name of the game is to basically prove the laws of thermodynamics, those undeniable constraints on all macroscopic phenomena, by combining the properties of atoms and the laws of microscopic motion via some sexy results from probability theory. Beholding its results truly takes the breath away!&amp;nbsp;&lt;a href=&quot;#fnref2&quot; rev=&quot;footnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;

&lt;li id=&quot;fn3&quot;&gt;
&lt;p&gt;If you&amp;#39;re unfamilair with index notation, I strongly insist that you learn it. Two rules to never forget:1) Conservation of indices: All free indices that appear on the left hand side must appear on the right hand side. 2) Repeated indices denote taking a product and summing over all permissable values of the index. In 2d for instance \(a_ib_i = a_1b_1 + a_2b_2\).&amp;nbsp;&lt;a href=&quot;#fnref3&quot; rev=&quot;footnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;

&lt;li id=&quot;fn4&quot;&gt;
&lt;p&gt;All this only applies to Newtonian non-polar fluids. If the fluid is non-polar, there can be an internal angular momentum set up and we will need to supplement our equations with the conservation of angular momentum. The NS equations don&amp;#39;t apply to these.&amp;nbsp;&lt;a href=&quot;#fnref4&quot; rev=&quot;footnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;

&lt;/ol&gt;
&lt;/div&gt;
</content>
 </entry>
 

</feed>
